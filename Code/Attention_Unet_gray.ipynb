{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 불러오기\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## 라이브러리 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경로 설정\n",
    "train_mask_paths = glob.glob(\"../Attention_Unet_gray/Comprehensive data/train/mask/*.png\")\n",
    "train_original_paths = glob.glob('../Attention_Unet_gray/Comprehensive data/train/original/*.png')\n",
    "train_mask_list = []\n",
    "train_original_list = []\n",
    "dir_save_train_np = '../Attention_Unet_gray/Comprehensive data/train_np'\n",
    "\n",
    "#지정한 폴더가 없으면 생성\n",
    "if not os.path.exists(dir_save_train_np):\n",
    "    os.makedirs(dir_save_train_np)\n",
    "\n",
    "#이미지 데이터를 256*256 크기로 전처리 후 npy 배열 형태로 지정폴더에 저장.\n",
    "for train_mask_path, train_original_path in zip(train_mask_paths, train_original_paths):\n",
    "    train_mask_list.append(np.array(Image.open(train_mask_path).resize((256, 256)).convert('L')))\n",
    "    train_original_list.append(np.array(Image.open(train_original_path).resize((256, 256)).convert('L')))\n",
    "\n",
    "for i, (train_mask, train_original) in enumerate(zip(train_mask_list, train_original_list)):\n",
    "\n",
    "    label_ = np.asarray(train_mask)\n",
    "    input_ = np.asarray(train_original)\n",
    "\n",
    "    np.save(os.path.join(dir_save_train_np, 'label_%03d.npy' % i), label_)\n",
    "    np.save(os.path.join(dir_save_train_np, 'input_%03d.npy' % i), input_)\n",
    "    \n",
    "val_mask_paths = glob.glob('../Attention_Unet_gray/Comprehensive data/val/mask/*.png')\n",
    "val_original_paths = glob.glob('../Attention_Unet_gray/Comprehensive data/val/original/*.png')\n",
    "val_mask_list = []\n",
    "val_original_list = []\n",
    "dir_save_val_np = '../Attention_Unet_gray/Comprehensive data/val_np'\n",
    "if not os.path.exists(dir_save_val_np):\n",
    "    os.makedirs(dir_save_val_np)\n",
    "\n",
    "for val_mask_path, val_original_path in zip(val_mask_paths, val_original_paths):\n",
    "    val_mask_list.append(np.array(Image.open(val_mask_path).resize((256, 256)).convert('L')))\n",
    "    val_original_list.append(np.array(Image.open(val_original_path).resize((256, 256)).convert('L')))\n",
    "\n",
    "for i, (val_mask, val_original) in enumerate(zip(val_mask_list, val_original_list)):\n",
    "\n",
    "    label_ = np.asarray(val_mask)\n",
    "    input_ = np.asarray(val_original)\n",
    "\n",
    "    np.save(os.path.join(dir_save_val_np, 'label_%03d.npy' % i), label_)\n",
    "    np.save(os.path.join(dir_save_val_np, 'input_%03d.npy' % i), input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(AttentionGate, self).__init__()\n",
    "\n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_c)\n",
    "        )\n",
    "        self.Ws = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_c)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, g, s):\n",
    "        Wg = self.Wg(g)\n",
    "        Ws = self.Ws(s)\n",
    "        out = self.relu(Wg + Ws)\n",
    "        out = self.output(out)\n",
    "        return out * s\n",
    "\n",
    "class ConvBNRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Attention_Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Unet, self).__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList([\n",
    "            ConvBNRelu(1, 64), ConvBNRelu(64, 64), nn.MaxPool2d(kernel_size=2),\n",
    "            ConvBNRelu(64, 128), ConvBNRelu(128, 128), nn.MaxPool2d(kernel_size=2),\n",
    "            ConvBNRelu(128, 256), ConvBNRelu(256, 256), nn.MaxPool2d(kernel_size=2),\n",
    "            ConvBNRelu(256, 512), ConvBNRelu(512, 512), nn.MaxPool2d(kernel_size=2),\n",
    "            ConvBNRelu(512, 1024)\n",
    "        ])\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            ConvBNRelu(1024, 512), nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2), AttentionGate(in_c=512, out_c=512),\n",
    "            ConvBNRelu(2 * 512, 512), ConvBNRelu(512, 256), nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2), AttentionGate(in_c=256, out_c=256),\n",
    "            ConvBNRelu(2 * 256, 256), ConvBNRelu(256, 128), nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2), AttentionGate(in_c=128, out_c=128),\n",
    "            ConvBNRelu(2 * 128, 128), ConvBNRelu(128, 64), nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2), AttentionGate(in_c=64, out_c=64),\n",
    "            ConvBNRelu(2 * 64, 64), nn.Conv2d(64, 1, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outs = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            encoder_outs.append(x)\n",
    "\n",
    "        for idx, decoder in enumerate(self.decoders):\n",
    "            if idx % 3 == 2:\n",
    "                x = decoder(x, encoder_outs[4 - idx // 3])\n",
    "            else:\n",
    "                x = decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# net = Attention_Unet().to(device)\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in net.state_dict():\n",
    "#     print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
    "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
    "\n",
    "        lst_label.sort()\n",
    "        lst_input.sort()\n",
    "\n",
    "        self.lst_label = lst_label\n",
    "        self.lst_input = lst_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
    "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
    "\n",
    "        # 정규화\n",
    "        label = label/255.0\n",
    "        input = input/255.0\n",
    "\n",
    "        # 이미지와 레이블의 차원 = 2일 경우(채널이 없을 경우, 흑백 이미지), 새로운 채널(축) 생성\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        # transform이 정의되어 있다면 transform을 거친 데이터를 불러옴\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
    "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "\n",
    "class Normalization(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        input = (input - self.mean) / self.std\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data\n",
    "\n",
    "class RandomFlip(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.flipud(label)\n",
    "            input = np.flipud(input)\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data\n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.flipud(label)\n",
    "            input = np.flipud(input)\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data\n",
    "    \n",
    "class RandomRotation(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        # 무작위로 회전 각도 생성\n",
    "        angle = transforms.RandomRotation.get_params((-45, 45))\n",
    "        \n",
    "        # 넘파이 배열로 변환\n",
    "        label = label.numpy()\n",
    "        input = input.numpy()\n",
    "\n",
    "        # 무작위 회전 적용\n",
    "        input_tensor_input = torch.from_numpy(input)\n",
    "        rotated_input_tensor = transforms.functional.rotate(input_tensor_input, angle)\n",
    "\n",
    "        # 무작위 회전 적용\n",
    "        input_tensor_label = torch.from_numpy(label)\n",
    "        rotated_label_tensor = transforms.functional.rotate(input_tensor_label, angle)\n",
    "\n",
    "        # 넘파이 배열로 변환\n",
    "        rotated_input = rotated_input_tensor.numpy()\n",
    "        rotated_label = rotated_label_tensor.numpy()\n",
    "\n",
    "        data = {'label': rotated_label, 'input': rotated_input}\n",
    "\n",
    "        return data\n",
    "\n",
    "#밝기, 대비, 채도, 색조 무작위 변환\n",
    "class RandomColorJitter(object):\n",
    "    def __init__(self):\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5\n",
    "        )\n",
    "\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        input_tensor = torch.from_numpy(input)\n",
    "        jittered_input_tensor = self.color_jitter(input_tensor)\n",
    "\n",
    "        # 라벨은 원본 유지\n",
    "        data = {'label': label, 'input': jittered_input_tensor.numpy()}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 네트워크 저장하기\n",
    "def save(ckpt_dir, net, optim, epoch, best=False):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    if best:\n",
    "        torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
    "                   \"%s/best_Attention_Unet_gray.pth\" % ckpt_dir)\n",
    "    #주석 해제하면 best epoch마다 가중치 새로 저장함.\n",
    "    # else:\n",
    "    #     torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
    "    #                \"%s/best_Attention_Unet_gray_epoch%d.pth\" % (ckpt_dir, epoch))\n",
    "\n",
    "def load(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst = [f for f in ckpt_lst if f.endswith(\".pth\")]\n",
    "    ckpt_lst = sorted(ckpt_lst, key=lambda f: int(''.join(filter(str.isdigit, f.split(\".\")[0]))) if f.split(\".\")[0].isdigit() else -1)\n",
    "\n",
    "    if not ckpt_lst:\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "\n",
    "    dict_model = torch.load(os.path.join(ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    \n",
    "    # 파일명에서 epoch 추출 시 예외 처리\n",
    "    try:\n",
    "        epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "    except IndexError:\n",
    "        epoch = 0\n",
    "\n",
    "    return net, optim, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epoch = 1000\n",
    "\n",
    "# Early stoping\n",
    "patience_limit = 15\n",
    "patience_check = 0\n",
    "\n",
    "base_dir = \"../Attention_Unet_gray\"\n",
    "data_dir = \"../Attention_Unet_gray/Comprehensive data\"\n",
    "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    Normalization(mean=(0.5), std=(0.225)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),  \n",
    "    RandomFlip(),\n",
    "    ToTensor(),\n",
    "    RandomRotation(),  # 무작위 회전\n",
    "    RandomColorJitter(),  # 색상 변화\n",
    "    \n",
    "])\n",
    "\n",
    "# DataLoader\n",
    "dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train_np'), transform=transform)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val_np'), transform=transform)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create Network\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Attention_Unet().to(device)\n",
    "\n",
    "# Define loss function\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Set Optimizer with Elastic Net regularization\n",
    "l1_lambda = 0  # L1 규제 강도 조절 (하이퍼파라미터)\n",
    "l2_lambda = 0.0001  # L2 규제 강도 조절 (하이퍼파라미터)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# # Set other ancillary variablesea\n",
    "num_data_train = len(dataset_train)\n",
    "num_data_val = len(dataset_val)\n",
    "\n",
    "num_batch_train = np.ceil(num_data_train / batch_size)\n",
    "num_batch_val = np.ceil(num_data_val / batch_size)\n",
    "\n",
    "# # Set other ancillary functions\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)\n",
    "\n",
    "# Load the trained model, if any\n",
    "st_epoch = 0\n",
    "#net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim) \n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# 학습률 모니터 함수 정의\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# IoU 계산 함수 정의\n",
    "def calculate_iou(prediction, ground_truth):\n",
    "    prediction = prediction.cpu().numpy()\n",
    "    ground_truth = ground_truth.cpu().numpy() \n",
    "    \n",
    "    intersection = np.logical_and(prediction, ground_truth)\n",
    "    union = np.logical_or(prediction, ground_truth)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(st_epoch + 1, num_epoch + 1):\n",
    "        net.train()\n",
    "        loss_arr = []\n",
    "        iou_arr = []\n",
    "\n",
    "        for batch, data in enumerate(loader_train, 1):\n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "\n",
    "            output = net(input)\n",
    "\n",
    "            # backward pass\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = fn_loss(output, label)\n",
    "\n",
    "            l1_loss = 0\n",
    "            for param in net.parameters():\n",
    "                l1_loss += torch.norm(param, p=1)  # 각 가중치의 L1 노름을 더함\n",
    "\n",
    "            loss = loss + l1_lambda * l1_loss + l2_lambda * loss  # Elastic Net 규제 항 추가\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            # Calculate the loss function\n",
    "            loss_arr += [loss.item()]\n",
    "\n",
    "            # Calculate the IoU\n",
    "            prediction = fn_class(output)\n",
    "            iou = calculate_iou(prediction, label)\n",
    "            iou_arr += [iou]\n",
    "\n",
    "            current_lr = get_lr(optim)\n",
    "            \n",
    "            print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f | IOU %.4f | Patience %d | lr %.7f\" %\n",
    "                    (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr), np.mean(iou_arr), patience_check, current_lr))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_loss_arr = []\n",
    "            iou_arr = []\n",
    "\n",
    "            for batch, data in enumerate(loader_val, 1):\n",
    "                # forward pass\n",
    "                label = data['label'].to(device)\n",
    "                input = data['input'].to(device)\n",
    "\n",
    "                output = net(input)\n",
    "\n",
    "                # Calculate the loss function\n",
    "                loss = fn_loss(output, label)\n",
    "\n",
    "                l1_loss = 0\n",
    "                for param in net.parameters():\n",
    "                    l1_loss += torch.norm(param, p=1)  # 각 가중치의 L1 노름을 더함\n",
    "\n",
    "                loss = loss + l1_lambda * l1_loss + l2_lambda * loss  # Elastic Net 규제 항 추가\n",
    "\n",
    "                val_loss_arr += [loss.item()]\n",
    "\n",
    "                # Calculate the IoU\n",
    "                prediction = fn_class(output)\n",
    "                iou = calculate_iou(prediction, label)\n",
    "                iou_arr += [iou]\n",
    "\n",
    "                print(\"VAILD: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f | IOU %.4f | Patience %d | lr %.7f\" %\n",
    "                    (epoch, num_epoch, batch, num_batch_val, np.mean(val_loss_arr), np.mean(iou_arr), patience_check, current_lr))\n",
    "\n",
    "        mean_loss = np.mean(val_loss_arr)\n",
    "\n",
    "        # Step the ReduceLROnPlateau scheduler with validation loss\n",
    "        val_loss = np.mean(val_loss_arr)\n",
    "        scheduler.step(val_loss)  # 스케줄러에 검증 손실 전달\n",
    "\n",
    "        # 조기 종료 기능 (Early stopping)\n",
    "        if mean_loss < best_loss:  # 손실이 개선되면\n",
    "            best_loss = mean_loss\n",
    "            patience_check = 0  # 개선이 있을 때마다 참을성 카운터를 리셋\n",
    "            save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch, best=True)\n",
    "        else:\n",
    "            patience_check += 1  # 손실이 개선되지 않으면 참을성 카운터 증가\n",
    "\n",
    "        if patience_check >= patience_limit:  # 조기 종료 조건이 충족되면         \n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infernece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "폴더에 있는 모든 이미지파일을 infernece하여 npy, png 파일로 저장\n",
    "\n",
    "데이터 로더, 트랜스폼 Label만 할 수 있게 수정하여 다시 실행시켜줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#npy 파일로 전처리\n",
    "\n",
    "infer_data_paths = glob.glob('../Attention_Unet_gray/Thyroid_image/*png')\n",
    "\n",
    "dir_save_infer_np = '../Attention_Unet_gray/Infer_npy_data'\n",
    "\n",
    "if not os.path.exists(dir_save_infer_np):\n",
    "    os.makedirs(dir_save_infer_np)\n",
    "\n",
    "for i, infer_data_path in enumerate(infer_data_paths):\n",
    "    infer_data = np.array(Image.open(infer_data_path).resize((256, 256)).convert('L'))\n",
    "    input_ = np.asarray(infer_data)\n",
    "    np.save(os.path.join(dir_save_infer_np, 'input_%03d.npy' % i), input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 구현하기\n",
    "class Dataset_infer(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
    "\n",
    "        lst_input.sort()\n",
    "\n",
    "        self.lst_input = lst_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
    "\n",
    "        # 정규화\n",
    "        input = input/255.0\n",
    "\n",
    "        # 이미지와 레이블의 차원 = 2일 경우(채널이 없을 경우, 흑백 이미지), 새로운 채널(축) 생성\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input}\n",
    "\n",
    "        # transform이 정의되어 있다면 transform을 거친 데이터를 불러옴\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌스폼 구현하기\n",
    "class ToTensor_infer(object):\n",
    "    def __call__(self, data):\n",
    "        input = data['input']\n",
    "\n",
    "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        data = {'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "\n",
    "class Normalization_infer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        input = data['input']\n",
    "        input = (input - self.mean) / self.std\n",
    "        data = {'input': input}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../Attention_Unet_gray/'\n",
    "infer_base_dir = '../Attention_Unet_gray/'\n",
    "infer_data_dir = '../Attention_Unet_gray/'\n",
    "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
    "\n",
    "transform = transforms.Compose([Normalization_infer(mean=0.5, std=0.5), ToTensor_infer()])\n",
    "\n",
    "dataset_test = Dataset_infer(data_dir=os.path.join(infer_data_dir, 'Infer_npy_data'), transform=transform)\n",
    "loader_test = DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# 결과 디렉토리 생성하기\n",
    "result_dir = os.path.join(infer_base_dir, 'result')\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(os.path.join(result_dir, 'png'))\n",
    "    os.makedirs(os.path.join(result_dir, 'numpy'))\n",
    "\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "# 그밖에 부수적인 variables 설정하기\n",
    "num_data_test = len(dataset_test)\n",
    "num_batch_test = np.ceil(num_data_test / batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "\n",
    "    for batch, data in enumerate(loader_test, 1):\n",
    "        \n",
    "        # forward pass\n",
    "        input = data['input'].to(device)\n",
    "        output = net(input)\n",
    "        \n",
    "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "        output = fn_tonumpy(fn_class(output))\n",
    "        \n",
    "        # 테스트 결과 저장하기\n",
    "        for j in range(input.shape[0]):\n",
    "            id = num_batch_test * (batch - 1) + j\n",
    "\n",
    "            #plt.imsave(os.path.join(result_dir, 'png', 'input_%04d.png' % id), input[j].squeeze(), cmap='gray')\n",
    "            plt.imsave(os.path.join(result_dir, 'png', 'output_%04d.png' % id), output[j].squeeze(), cmap='gray')\n",
    "\n",
    "            #np.save(os.path.join(result_dir, 'numpy', 'input_%04d.npy' % id), input[j].squeeze())\n",
    "            np.save(os.path.join(result_dir, 'numpy', 'output_%04d.npy' % id), output[j].squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
